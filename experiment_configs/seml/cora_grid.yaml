seml:
  conda_environment: 'scalable_graff'
  project_root_dir: '/nfs/homedirs/charpent/scalable-graff/src/'
  executable: 'run_seml.py'
  output_dir: '/nfs/homedirs/charpent/slurm-output'
  name: 'cora_grid'

slurm:
  experiments_per_job: 1
  sbatch_options:
    gres: 'gpu:1'     # num GPUs
    mem: 32G        # memory
    cpus-per-task: 1  # num cores
    time: '2-00:00'   # max time, D-HH:MM
    partition: 'gpu_all' # partition
#    exclude: gpu[18,19],mdsi-gpu[01,02]

###### BEGIN PARAMETER CONFIGURATION ######

fixed:
    dataset: "cora"
    dataset_directory: "/nfs/staff-hdd/charpent/datasets/"
    homophily: None

    ### Preprocessing Args
    preprocess: True
    dump: True
    adj_norm: "row"
#    num_powers: 5
#    directed: False
#    self_loops: True

    ### Model Args
    model: "sgraff"
#    hidden_dim: 512
#    num_layers: 2
#    dropout: .5
#    input_dropout: 0
    attention_dropout: 0.
    W_type: "diag_dom"
    omega_type: "identity"

    ### Training Args
#    lr: 1e-2
    weight_decay: 0
    num_epochs: 10000
    patience: 50
#    batch_size: 1024
    test_batch_size: 100000
    full_batch: False
    num_runs: 10

    ### System Args
    gpu_idx: 0
    num_workers: 0
    log: "INFO"
    profiler: False

random:
  samples: 5
  seed: 123

  dropout:
    type: uniform
    min: 0
    max: .5

  input_dropout:
    type: uniform
    min: 0
    max: .5

  lr:
    type: loguniform
    min: 1e-4
    max: 1e-2

grid:
  num_powers:
    type: "choice"
    options:
    - 1
    - 3
    - 5

  directed:
    type: "choice"
    options:
    - True
    - False

  self_loops:
    type: "choice"
    options:
    - True
    - False

  hidden_dim:
    type: "choice"
    options:
    - 64
    - 128
    - 256
    - 512

  num_layers:
    type: "choice"
    options:
    - 1
    - 2

  batch_size:
    type: "choice"
    options:
    - 256
    - 512
    - 1024